{"cells":[{"cell_type":"markdown","metadata":{},"source":["File Source: [Kaggle Notebook](https://www.kaggle.com/code/reganmaharjan/preparing-gpt-training-data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import numpy as np # linear algebra,\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv),\n","import tensorflow as tf\n","\n","import gc"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","import datasets #huggingface datasets\n","\n","print(\"Loading Dataset\")\n","data = datasets.load_dataset(\"raygx/Nepali-Extended-Text-Corpus\")\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##### BAG of words computation\n","from tqdm.auto import tqdm\n","\n","bag_of_words = set()\n","\n","for i in tqdm(range(0,data['train'].num_rows,50000)):\n","    j = i+100000\n","    j = j if j<data['train'].num_rows else data['train'].num_rows\n","\n","    bag_of_words = set(list(bag_of_words)+(\" \".join(data['train'].select(range(i,j))['text'])).split())\n","\n","\n","len(bag_of_words)  ## 4966875 words in the bag"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","context_length = 512\n","\n","print(\"Loading Tokenizer\")\n","\n","tokenizer = AutoTokenizer.from_pretrained('raygx/GPT2_Nepali_Tokenizer')\n","tokenizer.add_special_tokens({'pad_token': '<pad>','unk_token':'<unk>'})\n","tokenizer.model_max_length = context_length\n","\n","tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"266c8ae6-3b85-4265-98d1-3697aaaa9611","_uuid":"3c6dfa78-6e04-4a2f-95e2-aef5f15dce33","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%%time\n","\n","def preprocess_function(rows):\n","    concatenated_rows = tokenizer(tokenizer.bos_token.join(rows['text']))\n","\n","    total_length = len(concatenated_rows[list(concatenated_rows.keys())[0]])\n","    splits = int(total_length/context_length)    \n","    \n","    result = {\n","        k: np.array_split(t[:splits*context_length],splits)\n","        for k, t in concatenated_rows.items()\n","    }\n","    \n","    if total_length > splits*context_length:\n","        for k, t in concatenated_rows.items():\n","            result[k].append(concatenated_rows[k][-(context_length):])\n","    \n","    return result\n","\n","num_proc = os.cpu_count()\n","\n","print(\"Tokenizing the data\")\n","lm_data = data.map(\n","        preprocess_function,\n","        batched=True,\n","        num_proc=num_proc,\n","        remove_columns=data[\"train\"].column_names,\n","        batch_size=5000\n","    )\n","\n","print(lm_data)\n","lm_data.save_to_disk('GPT_Training_Data')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

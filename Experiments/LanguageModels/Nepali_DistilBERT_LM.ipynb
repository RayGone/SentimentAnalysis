{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training DistilBert model from huggingface using Nepali Dataset for Masked Language Modelling\n### Dataset used here is mixture of [Oscar Corpus](https://www.kaggle.com/datasets/hsebarp/oscar-corpus-nepali), [NepCov19Tweets dataset](https://www.kaggle.com/datasets/mathew11111/nepcov19tweets), [Nepali News dataset large](https://www.kaggle.com/datasets/ashokpant/nepali-news-dataset-large), [Nepali News dataset](https://www.kaggle.com/datasets/lotusacharya/nepalinewsdataset), [nepali-wikipedia-articles](https://www.kaggle.com/datasets/disisbig/nepali-wikipedia-articles), [urdu-nepali-parallel-corpus](https://www.kaggle.com/datasets/rtatman/urdunepali-parallel-corpus), [cc100](https://huggingface.co/datasets/cc100), [NepQuake15](github.com), [Sahitya](github.com) and health news datasets\n> ### I cleaned Oscar corpus (as much as possible) in this [Notebook](https://www.kaggle.com/code/reganmaharjan/cleaning-oscar-nepali-dataset).\n> ### The dataset in the input is merged and taken from this [Notebook](https://www.kaggle.com/code/reganmaharjan/tokenizer-nepcov19tweets/notebook).\n### Tokenizers are trained on this [Notebook](https://www.kaggle.com/code/reganmaharjan/nepali-tokenizers-4-transformers)\n","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nimport datasets\nfrom transformers import set_seed\n\nmodel_id = 'raygx/distilBERT-Nepali'\nrand_seed = 9\n\ndef seed_everything(seed=0):\n    random.seed(seed) # random\n    os.environ['PYTHONHASHSEED'] = str(seed) # python enviroment\n    np.random.seed(seed) # numpy\n    tf.keras.utils.set_random_seed(seed) # tensorflow\n    tf.random.set_seed(seed) # tensorflow\n    set_seed(seed) # hugging_face transformer\n\nseed_everything(rand_seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-13T04:36:21.503145Z","iopub.execute_input":"2023-07-13T04:36:21.504012Z","iopub.status.idle":"2023-07-13T04:36:34.687000Z","shell.execute_reply.started":"2023-07-13T04:36:21.503971Z","shell.execute_reply":"2023-07-13T04:36:34.685980Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"def pushToHub(thing,repo = None,token = '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'): \n    if not repo:\n        raise(Exception(\"Repo name not provided\"))\n        \n    thing_type = str(type(thing))\n    if not ('datasets' in thing_type or 'models' in thing_type or 'token' in thing_type):\n        raise(Exception(\"Either a Dataset or a Model can be pushed to hub.\\nConfirm what you are trying to push!\"))\n    # login require python > 3.9 \n    from huggingface_hub import login\n    login(token)\n\n    thing.push_to_hub(repo)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:48:13.165217Z","iopub.execute_input":"2023-07-13T08:48:13.166351Z","iopub.status.idle":"2023-07-13T08:48:13.173149Z","shell.execute_reply.started":"2023-07-13T08:48:13.166306Z","shell.execute_reply":"2023-07-13T08:48:13.172019Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"%%time\n\n## load from input\ndata = datasets.load_from_disk('/kaggle/input/preparing-bert-training-data/BERT_Training_Data')\n## save to working directory - input is readonly\ndata.save_to_disk('training_data')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:36:34.697954Z","iopub.execute_input":"2023-07-13T04:36:34.698697Z","iopub.status.idle":"2023-07-13T04:38:07.336012Z","shell.execute_reply.started":"2023-07-13T04:36:34.698659Z","shell.execute_reply":"2023-07-13T04:38:07.335045Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"CPU times: user 891 ms, sys: 8.21 s, total: 9.1 s\nWall time: 1min 32s\n","output_type":"stream"}]},{"cell_type":"code","source":"## load data from working directory\ndata = datasets.load_from_disk('training_data')\ndata = datasets.concatenate_datasets([data['train'],data['test']])\ndata","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:07.339428Z","iopub.execute_input":"2023-07-13T04:38:07.340312Z","iopub.status.idle":"2023-07-13T04:38:30.120356Z","shell.execute_reply.started":"2023-07-13T04:38:07.340251Z","shell.execute_reply":"2023-07-13T04:38:30.119224Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 861124\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Faced a bottleneck\n> Memory not enough to load all the data for MLM \n\n*Note: Remember to change bn variable passed to select()*\n\n*Note: The reason for the memory exhaustion was due to the batch size of training and validation set on model.fit()*\n","metadata":{}},{"cell_type":"code","source":"n_steps = 3\n\ndata_block_size = int(data.num_rows/n_steps)\na,b = 0,1  # run batch 4 # running all batch at once\nchunk = range(data_block_size*a,data_block_size*b)#data['train'].num_rows)#\n\nprint(\"Chunking data\",chunk,\"Batch:\",b,\"out of\",n_steps)\ndata.cleanup_cache_files()\ndata = data.select(chunk).shuffle(rand_seed).train_test_split(test_size=0.01)\ngc.collect()\ndata","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:30.121677Z","iopub.execute_input":"2023-07-13T04:38:30.122108Z","iopub.status.idle":"2023-07-13T04:38:30.872530Z","shell.execute_reply.started":"2023-07-13T04:38:30.122073Z","shell.execute_reply":"2023-07-13T04:38:30.871445Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Chunking data range(0, 287041) Batch: 1 out of 3\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 284170\n    })\n    test: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 2871\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"####This is for testing purpose only  -- comment when saving new version\n# data = datasets.DatasetDict({\n#     \"train\":data['train'].select(\n#         range(int(data['train'].num_rows/10))\n#     ),\n#     \"test\":data['test'].select(\n#         range(int(data['test'].num_rows/10))\n#     )\n# })\n# data","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:30.873993Z","iopub.execute_input":"2023-07-13T04:38:30.874446Z","iopub.status.idle":"2023-07-13T04:38:30.880222Z","shell.execute_reply.started":"2023-07-13T04:38:30.874411Z","shell.execute_reply":"2023-07-13T04:38:30.879346Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Loading Tokenizers**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncontext_length = 512\n\nprint(\"Loading Tokenizer\")\ntry:\n    if not a:\n        raise(Exception('Error')) ## Load new tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\nexcept:    \n    tokenizer = AutoTokenizer.from_pretrained('raygx/BERT_Nepali_Tokenizer')\n    tokenizer.model_max_length = context_length\n\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:30.881565Z","iopub.execute_input":"2023-07-13T04:38:30.882052Z","iopub.status.idle":"2023-07-13T04:38:32.525419Z","shell.execute_reply.started":"2023-07-13T04:38:30.882019Z","shell.execute_reply":"2023-07-13T04:38:32.524216Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Loading Tokenizer\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/146 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f3a5e0a788e46f29ef608a50b1e5f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f71d6ed311a4a08b995552b3a2ed016"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4f021572463467782c6cad509114658"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"PreTrainedTokenizerFast(name_or_path='raygx/BERT_Nepali_Tokenizer', vocab_size=50000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\nprint(\"Initializing Data Collator\")\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n                                                mlm=True, mlm_probability=0.25,\n                                                return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:32.526956Z","iopub.execute_input":"2023-07-13T04:38:32.527507Z","iopub.status.idle":"2023-07-13T04:38:33.223252Z","shell.execute_reply.started":"2023-07-13T04:38:32.527470Z","shell.execute_reply":"2023-07-13T04:38:33.222297Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Initializing Data Collator\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TFAutoModelForMaskedLM, AutoConfig\n\n## To change the size of embedding - N_EMBED must me properly divisible by the size N_HEAD value\nprint(\"Initializing Model\")\nmodel = TFAutoModelForMaskedLM.from_pretrained(model_id)\n\nmodel.resize_token_embeddings(len(tokenizer))\nprint(model.config)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:33.225138Z","iopub.execute_input":"2023-07-13T04:38:33.225859Z","iopub.status.idle":"2023-07-13T04:38:51.553563Z","shell.execute_reply.started":"2023-07-13T04:38:33.225820Z","shell.execute_reply":"2023-07-13T04:38:51.552581Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Initializing Model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b973273a733d4be2917068e4ea16433d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tf_model.h5:   0%|          | 0.00/360M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66721cfcb6ea49d0ab1c23b8e2465044"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFDistilBertForMaskedLM.\n\nAll the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at raygx/distilBERT-Nepali.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"DistilBertConfig {\n  \"_name_or_path\": \"raygx/distilBERT-Nepali\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 2,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.30.2\",\n  \"vocab_size\": 50000\n}\n\nModel: \"tf_distil_bert_for_masked_lm\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMai  multiple                 81321984  \n nLayer)                                                         \n                                                                 \n vocab_transform (Dense)     multiple                  590592    \n                                                                 \n vocab_layer_norm (LayerNorm  multiple                 1536      \n alization)                                                      \n                                                                 \n vocab_projector (TFDistilBe  multiple                 38844752  \n rtLMHead)                                                       \n                                                                 \n=================================================================\nTotal params: 81,964,112\nTrainable params: 81,964,112\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Preparing Training and Testing sets to TRAIN the MODEL\")\ntf_train_set = model.prepare_tf_dataset(\n    data[\"train\"],\n    shuffle=True,\n    batch_size=16,\n    collate_fn=data_collator,\n)\n\ntf_test_set = model.prepare_tf_dataset(\n    data[\"test\"],\n    shuffle=False,\n    batch_size=16,\n    collate_fn=data_collator,\n)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:51.557391Z","iopub.execute_input":"2023-07-13T04:38:51.558036Z","iopub.status.idle":"2023-07-13T04:38:52.449414Z","shell.execute_reply.started":"2023-07-13T04:38:51.557998Z","shell.execute_reply":"2023-07-13T04:38:52.448240Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"Preparing Training and Testing sets to TRAIN the MODEL\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"90"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import create_optimizer\n\nnum_train_steps = len(tf_train_set)\noptimizer, schedule = create_optimizer(\n    init_lr=5e-5,\n    num_warmup_steps=1_000,\n    num_train_steps=num_train_steps,\n    weight_decay_rate=0.01,\n)\nmodel.compile(optimizer=optimizer)\n\n# Train in mixed-precision float16\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:52.450837Z","iopub.execute_input":"2023-07-13T04:38:52.451651Z","iopub.status.idle":"2023-07-13T04:38:52.476624Z","shell.execute_reply.started":"2023-07-13T04:38:52.451615Z","shell.execute_reply":"2023-07-13T04:38:52.475686Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"%%time\n\nprint(\"Training the model\")\nhistory = model.fit(x=tf_train_set, \n          validation_data=tf_test_set,\n          #verbose=1,\n          epochs=1)\n\nmodel.save_pretrained(model_id)\nprint(history.history)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T04:38:52.478098Z","iopub.execute_input":"2023-07-13T04:38:52.478523Z","iopub.status.idle":"2023-07-13T08:43:48.023713Z","shell.execute_reply.started":"2023-07-13T04:38:52.478489Z","shell.execute_reply":"2023-07-13T08:43:48.021625Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training the model\n17760/17760 [==============================] - 14693s 827ms/step - loss: 4.8605 - val_loss: 4.0510\n{'loss': [4.8605146408081055], 'val_loss': [4.051042556762695]}\nCPU times: user 3h 5min 2s, sys: 2min 27s, total: 3h 7min 29s\nWall time: 4h 4min 55s\n","output_type":"stream"}]},{"cell_type":"code","source":"# from seaborn import lineplot\n# from matplotlib import pyplot as plt\n\n# lineplot(history.history['loss'])\n# lineplot(history.history['val_loss'])\n\n# plt.plot()","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:43:48.029337Z","iopub.execute_input":"2023-07-13T08:43:48.032714Z","iopub.status.idle":"2023-07-13T08:43:48.039408Z","shell.execute_reply.started":"2023-07-13T08:43:48.032672Z","shell.execute_reply":"2023-07-13T08:43:48.038348Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import math\n\neval_loss = model.evaluate(tf_test_set)\nprint(f\"Perplexity: {math.exp(eval_loss):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:43:48.042160Z","iopub.execute_input":"2023-07-13T08:43:48.042655Z","iopub.status.idle":"2023-07-13T08:44:41.800464Z","shell.execute_reply.started":"2023-07-13T08:43:48.042621Z","shell.execute_reply":"2023-07-13T08:44:41.799338Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"180/180 [==============================] - 54s 297ms/step - loss: 4.0423\nPerplexity: 56.96\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\n\n# Deleting dataset directory \ndir_path = r\"/kaggle/working/training_data\"\nshutil.rmtree(dir_path, ignore_errors=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:44:41.802141Z","iopub.execute_input":"2023-07-13T08:44:41.802939Z","iopub.status.idle":"2023-07-13T08:44:41.809289Z","shell.execute_reply.started":"2023-07-13T08:44:41.802902Z","shell.execute_reply":"2023-07-13T08:44:41.808329Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"### Testing\nfrom transformers import FillMaskPipeline\n\ntokenizer.model_input_names = ['input_ids','attention_mask']\npipeline = FillMaskPipeline(model=model,tokenizer=tokenizer,device=1)\npipeline('नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच [MASK] वर्ष अघिदेखि')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:48:45.539251Z","iopub.execute_input":"2023-07-13T08:48:45.539666Z","iopub.status.idle":"2023-07-13T08:48:45.752983Z","shell.execute_reply.started":"2023-07-13T08:48:45.539637Z","shell.execute_reply":"2023-07-13T08:48:45.752075Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[{'score': 0.24879856407642365,\n  'token': 5493,\n  'token_str': 'सय',\n  'sequence': 'नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच सय वर्ष अघिदेखि'},\n {'score': 0.14424605667591095,\n  'token': 5175,\n  'token_str': 'वर्ष',\n  'sequence': 'नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच वर्ष वर्ष अघिदेखि'},\n {'score': 0.09164553135633469,\n  'token': 668,\n  'token_str': '०',\n  'sequence': 'नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच ० वर्ष अघिदेखि'},\n {'score': 0.061376817524433136,\n  'token': 5375,\n  'token_str': 'हजार',\n  'sequence': 'नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच हजार वर्ष अघिदेखि'},\n {'score': 0.030752407386898994,\n  'token': 673,\n  'token_str': '५',\n  'sequence': 'नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच ५ वर्ष अघिदेखि'}]"},"metadata":{}}]},{"cell_type":"code","source":"pushToHub(model,repo=model_id)\npushToHub(tokenizer,repo=model_id)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T08:48:53.771935Z","iopub.execute_input":"2023-07-13T08:48:53.772333Z","iopub.status.idle":"2023-07-13T08:49:01.270978Z","shell.execute_reply.started":"2023-07-13T08:48:53.772294Z","shell.execute_reply":"2023-07-13T08:49:01.269993Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
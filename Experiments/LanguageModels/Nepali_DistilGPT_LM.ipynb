{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training DistillGPT2 model from huggingface using Nepali Dataset for Causal Language Modelling\n### Dataset used here is mixture of [Oscar Corpus](https://www.kaggle.com/datasets/hsebarp/oscar-corpus-nepali), [NepCov19Tweets dataset](https://www.kaggle.com/datasets/mathew11111/nepcov19tweets), [Nepali News dataset large](https://www.kaggle.com/datasets/ashokpant/nepali-news-dataset-large), [Nepali News dataset](https://www.kaggle.com/datasets/lotusacharya/nepalinewsdataset), [nepali-wikipedia-articles](https://www.kaggle.com/datasets/disisbig/nepali-wikipedia-articles), [urdu-nepali-parallel-corpus](https://www.kaggle.com/datasets/rtatman/urdunepali-parallel-corpus), [cc100](https://huggingface.co/datasets/cc100), [NepQuake15](github.com), [Sahitya](github.com) and health news datasets\n> ### I cleaned Oscar corpus (as much as possible) in this [Notebook](https://www.kaggle.com/code/reganmaharjan/cleaning-oscar-nepali-dataset).\n> ### The dataset in the input is merged and taken from this [Notebook](https://www.kaggle.com/code/reganmaharjan/tokenizer-nepcov19tweets/notebook).\n### Tokenizers are trained on this [Notebook](https://www.kaggle.com/code/reganmaharjan/nepali-tokenizers-4-transformers)","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nimport datasets\nfrom transformers import set_seed\n\nmodel_id = 'raygx/distilGPTBhai'\n\nrand_seed = 9\n\ndef seed_everything(seed=0):\n    random.seed(seed) # random\n    os.environ['PYTHONHASHSEED'] = str(seed) # python enviroment\n    np.random.seed(seed) # numpy\n    tf.keras.utils.set_random_seed(seed) # tensorflow\n    tf.random.set_seed(seed) # tensorflow\n    set_seed(seed) # hugging_face transformer\n\nseed_everything(rand_seed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-13T03:54:19.412940Z","iopub.execute_input":"2023-07-13T03:54:19.413790Z","iopub.status.idle":"2023-07-13T03:54:32.145905Z","shell.execute_reply.started":"2023-07-13T03:54:19.413729Z","shell.execute_reply":"2023-07-13T03:54:32.144882Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def pushToHub(thing,repo = None,token = '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'): \n    if not repo:\n        raise(Exception(\"Repo name not provided\"))\n        \n    thing_type = str(type(thing))\n    if not ('datasets' in thing_type or 'models' in thing_type):\n        raise(Exception(\"Either a Dataset or a Model can be pushed to hub.\\nConfirm what you are trying to push!\"))\n    # login require python > 3.9 \n    from huggingface_hub import login\n    login(token)\n\n    thing.push_to_hub(repo)","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:54:32.147656Z","iopub.execute_input":"2023-07-13T03:54:32.148533Z","iopub.status.idle":"2023-07-13T03:54:32.156478Z","shell.execute_reply.started":"2023-07-13T03:54:32.148504Z","shell.execute_reply":"2023-07-13T03:54:32.155446Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%time\n\n## load from input\ndata = datasets.load_from_disk('/kaggle/input/preparing-gpt-training-data/GPT_Training_Data')\n## save to working directory - input is readonly\ndata.save_to_disk('training_data')","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:54:32.158390Z","iopub.execute_input":"2023-07-13T03:54:32.159180Z","iopub.status.idle":"2023-07-13T03:56:16.293442Z","shell.execute_reply.started":"2023-07-13T03:54:32.159146Z","shell.execute_reply":"2023-07-13T03:56:16.291509Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"CPU times: user 965 ms, sys: 10.3 s, total: 11.3 s\nWall time: 1min 44s\n","output_type":"stream"}]},{"cell_type":"code","source":"## load data from working directory\ndata = datasets.load_from_disk('training_data')\ndata = datasets.concatenate_datasets([data['train'],data['test']])\ndata","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:56:16.295881Z","iopub.execute_input":"2023-07-13T03:56:16.297108Z","iopub.status.idle":"2023-07-13T03:56:44.851913Z","shell.execute_reply.started":"2023-07-13T03:56:16.297068Z","shell.execute_reply":"2023-07-13T03:56:44.850732Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 2903175\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Faced a bottleneck\n> Datasize is too large hardly one single epoch was complete in 12hr session runtime provided by kaggle.<br>\n> So, thinking of training on 1/16th the data size at a time.\n\n*Note: Remember to change bn variable passed to select()*\n\n*Note: The reason for the memory exhaustion was due to the batch size of training and validation set on model.fit()*\n","metadata":{}},{"cell_type":"code","source":"n_steps = 8\n\ndata_block_size = int(data.num_rows/n_steps)\na,b = 2,3  # run batch 4 # running all batch at once\nchunk = range(data_block_size*a,data_block_size*b)#data['train'].num_rows)#\n\nprint(\"Chunking data\",chunk,\"Batch:\",b,\"out of\",n_steps)\ndata.cleanup_cache_files()\ndata = data.select(chunk).shuffle(rand_seed).train_test_split(test_size=0.01)\ngc.collect()\ndata","metadata":{"execution":{"iopub.status.busy":"2023-07-13T03:56:44.853233Z","iopub.execute_input":"2023-07-13T03:56:44.853647Z","iopub.status.idle":"2023-07-13T03:56:45.868702Z","shell.execute_reply.started":"2023-07-13T03:56:44.853601Z","shell.execute_reply":"2023-07-13T03:56:45.867665Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Chunking data range(725792, 1088688) Batch: 3 out of 8\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 359267\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 3629\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ncontext_length = 512\n\nprint(\"Loading Tokenizer\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\nexcept:    \n    tokenizer = AutoTokenizer.from_pretrained('raygx/GPT2_Nepali_Tokenizer')\n    tokenizer.add_special_tokens({'pad_token': '<pad>','unk_token':'<unk>'})\n    tokenizer.model_max_length = context_length\n\ntokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\nprint(\"Initializing Data Collator\")\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n                                                mlm=False, \n                                                return_tensors=\"tf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForCausalLM, AutoConfig\n\n## To change the size of embedding - N_EMBED must me properly divisible by the size N_HEAD value\nprint(\"Initializing Model\")\ntry:\n    model = TFAutoModelForCausalLM.from_pretrained(model_id)\n    print('Loading Pretrained')\nexcept:    \n    model = TFAutoModelForCausalLM.from_pretrained(\"raygx/Nepali-DistilGPT2\",\n                                            bos_token_id=tokenizer.bos_token_id,\n                                            eos_token_id=tokenizer.eos_token_id,\n                                            pad_token_id=tokenizer.pad_token_id)\n    print('Loading Previous Checkpoint')\n\nmodel.resize_token_embeddings(len(tokenizer))\nprint(model.config)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Preparing Training and Testing sets to TRAIN the MODEL\")\ntf_train_set = model.prepare_tf_dataset(\n    data[\"train\"],\n    shuffle=True,\n    batch_size=16,\n    collate_fn=data_collator,\n)\n\ntf_test_set = model.prepare_tf_dataset(\n    data[\"test\"],\n    shuffle=False,\n    batch_size=16,\n    collate_fn=data_collator,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import create_optimizer\n\nnum_train_steps = len(tf_train_set)\noptimizer, schedule = create_optimizer(\n    init_lr=5e-5,\n    num_warmup_steps=1_000,\n    num_train_steps=num_train_steps,\n    weight_decay_rate=0.01,\n)\nmodel.compile(optimizer=optimizer)\n\n# Train in mixed-precision float16\ntf.keras.mixed_precision.set_global_policy(\"mixed_float16\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"%%time\n\nprint(\"Training the model\")\nhistory = model.fit(x=tf_train_set, \n          validation_data=tf_test_set,\n          verbose=2,\n          epochs=1)\n\nmodel.save_pretrained(model_id)\nprint(history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from seaborn import lineplot\n# from matplotlib import pyplot as plt\n\n# lineplot(history.history['loss'])\n# lineplot(history.history['val_loss'])\n\n# plt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\neval_loss = model.evaluate(tf_test_set)\nprint(f\"Perplexity: {math.exp(eval_loss):.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pushToHub(model,repo=model_id)\npushToHub(tokenizer,repo=model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nshutil.rmtree('/kaggle/working/training_data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
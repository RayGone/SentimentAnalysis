{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayGone/SentimentAnalysis/blob/phase1/Experiments/BERT%26GPT_Embedding_As_Feature_4SA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZzHsFXBbOkw"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tokenizers datasets huggingface_hub --quiet\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import set_seed\n",
        "\n",
        "rand_seed = 9\n",
        "\n",
        "def seed_everything(seed=0):\n",
        "    random.seed(seed) # random\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed) # python enviroment\n",
        "    np.random.seed(seed) # numpy\n",
        "    tf.keras.utils.set_random_seed(seed) # tensorflow\n",
        "    tf.random.set_seed(seed) # tensorflow\n",
        "    set_seed(seed) # hugging_face transformer\n",
        "\n",
        "seed_everything(rand_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Transformers"
      ],
      "metadata": {
        "id": "NG05ycyEL3ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, BertTokenizerFast, TFAutoModel\n",
        "import datasets\n",
        "\n",
        "gptModel = 'raygx/Nepali-GPT2-CausalLM'\n",
        "gptTokenizer = PreTrainedTokenizerFast.from_pretrained(gptModel,padding_side='left')\n",
        "gptModel = TFAutoModel.from_pretrained(gptModel)\n",
        "\n",
        "bertModel = 'Shushant/nepaliBERT'\n",
        "bertTokenizer = BertTokenizerFast.from_pretrained(bertModel)\n",
        "bertModel = TFAutoModel.from_pretrained(bertModel,from_pt=True)\n",
        "\n",
        "gptModel.config, bertModel.config"
      ],
      "metadata": {
        "id": "m8vLKWOLq0Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "gJppNsMYL7u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"#######################Using NepCov19Tweets#########################\")\n",
        "data = datasets.load_dataset(\"raygx/NepCov19TweetsPlus\")\n",
        "\n",
        "data = data.rename_columns({\"Sentiment\":\"label\",\"Sentences\":\"text\"})\n",
        "data"
      ],
      "metadata": {
        "id": "KwnwTdn_53Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.shuffle(rand_seed)\n",
        "data = data['train'].train_test_split(test_size=0.2)\n",
        "data"
      ],
      "metadata": {
        "id": "qu_hGjt27PnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareLabels(row):\n",
        "    if row['label'] == -1:\n",
        "        row['label'] = 2\n",
        "\n",
        "    return row\n",
        "\n",
        "data = data.map(\n",
        "        prepareLabels,\n",
        "        num_proc=4)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "HuIlfjXh7UAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Embedding Layer"
      ],
      "metadata": {
        "id": "_tgZPTg2L_e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class GPTEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, model,tokenizer, max_token_len=128,padding='max_length',trailing_context=4,truncation=True):\n",
        "    super().__init__()\n",
        "    self.embedding = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.tokenizer.padding_side = 'left'\n",
        "    self.max_token = max_token_len\n",
        "    self.padding = padding\n",
        "    self.truncation = truncation\n",
        "    self.trailing_context = trailing_context\n",
        "    self.trainable=False\n",
        "\n",
        "  def call(self, x):\n",
        "    embeddings = self.embedding(\n",
        "                  self.tokenizer(x,padding=self.padding,truncation=self.truncation,max_length=self.max_token,return_tensors='tf')\n",
        "                )[0][:,-self.trailing_context:,:]\n",
        "\n",
        "    return tf.reduce_logsumexp(embeddings,axis=1)\n",
        "\n",
        "class BERTEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, model,tokenizer, max_token_len=128,padding='max_length',truncation=True):\n",
        "    super().__init__()\n",
        "    self.embedding = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token = max_token_len\n",
        "    self.padding = padding\n",
        "    self.truncation = truncation\n",
        "    self.trainable=False\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.embedding(self.tokenizer(x,padding=self.padding,truncation=self.truncation,max_length=self.max_token,return_tensors='tf'))[1]"
      ],
      "metadata": {
        "id": "YuD-BoyG7ahO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.bert(tokenizer('बैंक तथा वित्तीय संस्थामा देखिएको विश्वासको',padding='max_length',truncation=True,return_tensors='tf',max_length=128))\n",
        "gpt_embd = GPTEmbedding(gptModel.transformer,gptTokenizer,max_token_len=100,trailing_context=1)\n",
        "bert_embd = BERTEmbedding(bertModel.bert,bertTokenizer,max_token_len=100)\n",
        "# embd([\"\",'बैंक तथा वित्तीय संस्थामा देखिएको विश्वासको','बैंक तथा वित्तीय संस्थामा देखिएको विश्वासको'])\n",
        "d_model = tf.keras.layers.Average()([gpt_embd([\"abcd\",'efgh']),bert_embd([\"abcd\",'efgh'])]).shape[1]\n",
        "d_model"
      ],
      "metadata": {
        "id": "N5ZLtQza7u0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Data Generator"
      ],
      "metadata": {
        "id": "2SNKBvuJMIoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, data, batch_size=32,shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.data = data\n",
        "        self.embeddings = np.empty((data.num_rows,768))\n",
        "        self.shuffle = shuffle\n",
        "        self.average = tf.keras.layers.Average()\n",
        "        self.is_first_epoch = True\n",
        "        self.on_epoch_end(is_first_epoch=self.is_first_epoch)\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.ceil(self.data.num_rows / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        return self.__data_generation(indexes)\n",
        "\n",
        "    def on_epoch_end(self,is_first_epoch=False):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(self.data.num_rows)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "        self.is_first_epoch = is_first_epoch\n",
        "\n",
        "\n",
        "    def __data_generation(self, indexes):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        chunk = self.data.select(indexes)\n",
        "\n",
        "        if self.is_first_epoch:\n",
        "          X = self.average([gpt_embd(chunk['text']), bert_embd(chunk['text'])])\n",
        "          self.embeddings[indexes] = X\n",
        "        else:\n",
        "          X = self.embeddings[indexes]\n",
        "\n",
        "        y = np.array(chunk['label'])\n",
        "        return X,y"
      ],
      "metadata": {
        "id": "jkr2xAKg8IXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "train_features = DataGenerator(data['train'],batch_size=32)\n",
        "test_features = DataGenerator(data['test'],shuffle=False)"
      ],
      "metadata": {
        "id": "srP-W6tp8rV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Head"
      ],
      "metadata": {
        "id": "2MC_H-PpML5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(d_model)),\n",
        "    tf.keras.layers.Dense(32,activation='tanh'),\n",
        "    tf.keras.layers.Dense(3,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.001\n",
        "      ),\n",
        "  loss='sparse_categorical_crossentropy',\n",
        "  metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PRdJHuhI8x4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Classification"
      ],
      "metadata": {
        "id": "QceKTK20MP_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_features,\n",
        "        epochs=100,\n",
        "        validation_data=test_features,\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(\n",
        "                            monitor='val_acc', patience=3,\n",
        "                            verbose=1, mode='auto',\n",
        "                            restore_best_weights=True)\n",
        "                        ])"
      ],
      "metadata": {
        "id": "hHaOiPzC-R11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "seaborn.lineplot(history.history['loss'])\n",
        "seaborn.lineplot(history.history['val_loss'])\n",
        "plt.title(\"Loss Graph\")\n",
        "plt.show()\n",
        "\n",
        "seaborn.lineplot(history.history['acc'])\n",
        "seaborn.lineplot(history.history['val_acc'])\n",
        "plt.title(\"Accuracy Graph\")"
      ],
      "metadata": {
        "id": "jIR4SrFz-yyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "QKWp7t2MMT6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from transformers import pipeline, TextClassificationPipeline\n",
        "\n",
        "print(\"Getting Test Prediction\")\n",
        "pred_labels = [np.argmax(tf.nn.softmax(model(tf.constant(x)).logits)) for x in tokenized_inputs['test']['input_ids']]\n",
        "\n",
        "actual_labels = data['test']['labels']"
      ],
      "metadata": {
        "id": "f7YyC_De-ttF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "print(\"F1-Score\",f1_score(actual_labels,pred_labels,average='weighted'))\n",
        "print(\"Precision-Score\",precision_score(actual_labels,pred_labels,average='weighted'))\n",
        "print(\"Recall-Score\",recall_score(actual_labels,pred_labels,average='weighted'))\n",
        "print(\"accuracy_Score\",accuracy_score(actual_labels,pred_labels))"
      ],
      "metadata": {
        "id": "gAM1On6w-v8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cmd = ConfusionMatrixDisplay(tf.math.confusion_matrix(actual_labels,pred_labels,num_classes=3).numpy())\n",
        "cmd.plot()"
      ],
      "metadata": {
        "id": "JSea9k73-x2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7o9tIjY2MXJk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
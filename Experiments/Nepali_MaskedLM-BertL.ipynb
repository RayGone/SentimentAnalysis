{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets evaluate huggingface-hub --quiet","metadata":{"execution":{"iopub.status.busy":"2023-07-08T03:11:19.362216Z","iopub.execute_input":"2023-07-08T03:11:19.362896Z","iopub.status.idle":"2023-07-08T03:11:30.972750Z","shell.execute_reply.started":"2023-07-08T03:11:19.362864Z","shell.execute_reply":"2023-07-08T03:11:30.971931Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport tensorflow as tf\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-08T03:11:30.974463Z","iopub.execute_input":"2023-07-08T03:11:30.974796Z","iopub.status.idle":"2023-07-08T03:11:39.589566Z","shell.execute_reply.started":"2023-07-08T03:11:30.974766Z","shell.execute_reply":"2023-07-08T03:11:39.588163Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/nepali-tokenizers/Nepali_Wordpiece.tokenizer\n/kaggle/input/nepali-tokenizers/Nepali_BPE.tokenizer\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Decided to use BPE tokenizer for following reasons\n1. As it was observed in [Tokenizer Training Notebook](https://www.kaggle.com/code/reganmaharjan/tokenizer-nepcov19tweets), BPE is faster than WordPiece.\n2. As it was observed in [Testing Tokenizer](https://www.kaggle.com/code/reganmaharjan/testing-tokenizer-nepali/) word breaks by WordPiece are in unusual place than in BPE. Though the observation is not exhaustive and tokenizer vocab is not scanned completely.\n3. and surprisinlgy the token ids for the tokens in vocab are almost identical in both BPE and Wordpiece.","metadata":{}},{"cell_type":"code","source":"%%time\nimport datasets #huggingface datasets\n\nprint(\"Loading Dataset\")\ndata1 = datasets.load_dataset(\"raygx/Nepali-Text-Corpus\")\ndata1 = data1.filter(lambda x: x['text']!=None,num_proc=4)\nprint(data1)\ndata2 = datasets.load_dataset(\"cc100\", lang=\"ne\")\nprint(data2)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T04:23:01.292885Z","iopub.execute_input":"2023-07-08T04:23:01.294690Z","iopub.status.idle":"2023-07-08T04:23:10.575207Z","shell.execute_reply.started":"2023-07-08T04:23:01.294598Z","shell.execute_reply":"2023-07-08T04:23:10.573509Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Loading Dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b17ac8c503469d8fc99080d1354b30"}},"metadata":{}},{"name":"stdout","text":"     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/474 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a15d374a8e444434beba53dcf6bbe12a"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/474 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7380fddcfad454ea7e5f1cbf88e0bf5"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/474 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd24ff8ca7374697a3b14b8e5935c27c"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/474 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4715ba09c084422d85d6c69623e5f042"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1895289\n    })\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10feba08e0e546a79d5e22c93bf54af8"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'text'],\n        num_rows: 12732810\n    })\n})\nCPU times: user 1.51 s, sys: 416 ms, total: 1.92 s\nWall time: 9.27 s\n","output_type":"stream"}]},{"cell_type":"code","source":"data = datasets.concatenate_datasets([data1['train'], data2['train']])\ndata = data.shuffle(999).train_test_split(test_size=0.001)\ngc.collect()\ndata","metadata":{"execution":{"iopub.status.busy":"2023-07-08T04:23:10.577783Z","iopub.execute_input":"2023-07-08T04:23:10.578192Z","iopub.status.idle":"2023-07-08T04:23:20.375573Z","shell.execute_reply.started":"2023-07-08T04:23:10.578159Z","shell.execute_reply":"2023-07-08T04:23:20.373972Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'id'],\n        num_rows: 14613470\n    })\n    test: Dataset({\n        features: ['text', 'id'],\n        num_rows: 14629\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# #### BAG of words computation\n# from tqdm.auto import tqdm\n\n# bag_of_words = set()\n\n# for i in tqdm(range(0,data['train'].num_rows,50000)):\n#     j = i+100000\n#     j = j if j<data['train'].num_rows else data['train'].num_rows\n    \n#     bag_of_words = set(list(bag_of_words)+(\" \".join(data['train'].select(range(i,j))['text'])).split())\n    \n\n# len(bag_of_words)  ## 4966875 words in the bag","metadata":{"execution":{"iopub.status.busy":"2023-07-08T04:23:20.377447Z","iopub.execute_input":"2023-07-08T04:23:20.377904Z","iopub.status.idle":"2023-07-08T04:42:49.114388Z","shell.execute_reply.started":"2023-07-08T04:23:20.377865Z","shell.execute_reply":"2023-07-08T04:42:49.112539Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6fa81678cb4ad6825a3148e3d45176"}},"metadata":{}},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"4966875"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\nprint(\"Initializing tokenizer as PreTrainedTokenizerFast\")\ntokenizer = BertTokenizerFast.from_pretrained('raygx/Nepali-GPT2-CausalLM')\ntokenizer.add_special_tokens({'pad_token': '[PAD]',\"eos_token\": \"[SEP]\", \"bos_token\":\"[CLS]\", \"mask_token\":\"[MASK]\"})","metadata":{"execution":{"iopub.status.busy":"2023-07-08T04:47:17.850211Z","iopub.execute_input":"2023-07-08T04:47:17.850616Z","iopub.status.idle":"2023-07-08T04:47:18.107406Z","shell.execute_reply.started":"2023-07-08T04:47:17.850591Z","shell.execute_reply":"2023-07-08T04:47:18.105930Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \nThe class this function is called from is 'BertTokenizerFast'.\n","output_type":"stream"},{"name":"stdout","text":"Initializing tokenizer as PreTrainedTokenizerFast\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(rows):\n    return tokenizer(rows['text'])\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T04:53:19.217717Z","iopub.execute_input":"2023-07-08T04:53:19.218186Z","iopub.status.idle":"2023-07-08T04:53:19.224167Z","shell.execute_reply.started":"2023-07-08T04:53:19.218152Z","shell.execute_reply":"2023-07-08T04:53:19.222591Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(\"Tokenizing the data\")\ntokenized_inputs = data.map(\n    preprocess_function,\n    batched=True,\n    num_proc=4,\n    remove_columns=data[\"train\"].column_names\n)\n# tokenized_inputs = tokenized_inputs.remove_columns(['token_type_ids'])\ntokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-07-08T04:53:33.559206Z","iopub.execute_input":"2023-07-08T04:53:33.559690Z","iopub.status.idle":"2023-07-08T05:09:48.780445Z","shell.execute_reply.started":"2023-07-08T04:53:33.559651Z","shell.execute_reply":"2023-07-08T05:09:48.778795Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Tokenizing the data\n     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0ad7b2f428473fa5b1dbdbefdaa141"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de898052bbd47cf9c415b4fbc3fa6f0"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eee3927118e48b4a20f8cebdbb0a450"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2342d88778684442a60d0ed1fd5a2dcf"}},"metadata":{}},{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89b8d8b96234140872200ad2f347494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae6604a57e44ffaba3412397324bb7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a216d6538d451cb54d1182933a8b9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf75a6973ea4b12b833fd8321a91611"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 31.4 s, sys: 10.4 s, total: 41.8 s\nWall time: 16min 15s\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 14613470\n    })\n    test: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 14629\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"block_size = 128\ngc.collect()\n\ndef group_texts(rows):\n    # Concatenate all texts.\n    concatenated_rows = {k: sum(rows[k], []) for k in rows.keys()}\n    total_length = len(concatenated_rows[list(rows.keys())[0]])\n    remainder = total_length\n    \n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n        remainder -=total_length\n        \n    # Split by chunks of block_size.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_rows.items()\n    }\n    \n    if(remainder):\n        for k in result.keys():\n            result[k].append(concatenated_rows[k][-128:])\n        \n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:09:56.292488Z","iopub.execute_input":"2023-07-08T05:09:56.292949Z","iopub.status.idle":"2023-07-08T05:09:57.175194Z","shell.execute_reply.started":"2023-07-08T05:09:56.292907Z","shell.execute_reply":"2023-07-08T05:09:57.173718Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"%%time\nprint(\"Grouping Tokens to Model Input Size\")\nlm_data = tokenized_inputs.map(group_texts, batched=True, num_proc=4)\nlm_data","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:10:02.298023Z","iopub.execute_input":"2023-07-08T05:10:02.298456Z","iopub.status.idle":"2023-07-08T05:35:16.141678Z","shell.execute_reply.started":"2023-07-08T05:10:02.298425Z","shell.execute_reply":"2023-07-08T05:35:16.140693Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Grouping Tokens to Model Input Size\n     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a5f7a87569f42569cb5a2deace579c5"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5592e8a454074cb980e44cbd37eea0f5"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b8215935cb54e92a7390bcd3b838f87"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/3654 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"637b451f61b94ec8905668168d60d041"}},"metadata":{}},{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a7c65dd3a34e27b2abf04bb31e6b2a"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1939d05526ae4f3cbee3da7f90e8e4be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea3139b333794c96830fea44c101d62c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eab7210f46744f186d4fa7b89e415ab"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 31.1 s, sys: 10.3 s, total: 41.4 s\nWall time: 25min 13s\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3398000\n    })\n    test: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 3342\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\nprint(\"Initializing Data Collator\")\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n                                                mlm=True, mlm_probability=0.25,\n                                                return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:36:07.711926Z","iopub.execute_input":"2023-07-08T05:36:07.712285Z","iopub.status.idle":"2023-07-08T05:36:07.719845Z","shell.execute_reply.started":"2023-07-08T05:36:07.712262Z","shell.execute_reply":"2023-07-08T05:36:07.718245Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Initializing Data Collator\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TFAutoModelForMaskedLM, AutoConfig\n\nmodel = TFAutoModelForMaskedLM.from_pretrained(\"bert-large-uncased\",\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.pad_token_id)\n\nmodel.resize_token_embeddings(len(tokenizer))\nprint(model.config)\ngc.collect()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:35:17.229508Z","iopub.execute_input":"2023-07-08T05:35:17.229844Z","iopub.status.idle":"2023-07-08T05:36:07.709330Z","shell.execute_reply.started":"2023-07-08T05:35:17.229818Z","shell.execute_reply":"2023-07-08T05:36:07.708053Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a09d8d15c5a4915b7c9f1e231bd5065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tf_model.h5:   0%|          | 0.00/1.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd3b43ac98884b6696d8dfc9eee33856"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFBertForMaskedLM.\n\nAll the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"BertConfig {\n  \"_name_or_path\": \"bert-large-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 1,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 2,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.28.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 50000\n}\n\nModel: \"tf_bert_for_masked_lm\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bert (TFBertMainLayer)      multiple                  354037760 \n                                                                 \n mlm___cls (TFBertMLMHead)   multiple                  52830032  \n                                                                 \n=================================================================\nTotal params: 355,139,408\nTrainable params: 355,139,408\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import create_optimizer, AdamWeightDecay\n\noptimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.0001)\nmodel.compile(optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:40:35.668554Z","iopub.execute_input":"2023-07-08T05:40:35.669058Z","iopub.status.idle":"2023-07-08T05:40:35.701598Z","shell.execute_reply.started":"2023-07-08T05:40:35.669025Z","shell.execute_reply":"2023-07-08T05:40:35.699900Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Preparing Training and Testing sets to TRAIN the MODEL\")\ntf_train_set = model.prepare_tf_dataset(\n    lm_data[\"train\"],\n    shuffle=True,\n    batch_size=16,\n    collate_fn=data_collator,\n)\n\ntf_test_set = model.prepare_tf_dataset(\n    lm_data[\"test\"],\n    shuffle=False,\n    batch_size=16,\n    collate_fn=data_collator,\n)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:38:29.652483Z","iopub.execute_input":"2023-07-08T05:38:29.653917Z","iopub.status.idle":"2023-07-08T05:38:31.113220Z","shell.execute_reply.started":"2023-07-08T05:38:29.653863Z","shell.execute_reply":"2023-07-08T05:38:31.111738Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"Preparing Training and Testing sets to TRAIN the MODEL\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"%%time\n\nprint(\"Training the model\")\nhistory = model.fit(x=tf_train_set, \n          validation_data=tf_test_set,\n          verbose=1,\n          epochs=5)\nmodel.save_pretrained(\"/kaggle/working/LBert-nepali-maskedlm\")\nprint(history.history)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T05:40:45.441433Z","iopub.execute_input":"2023-07-08T05:40:45.441872Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training the model\nEpoch 1/5\n     3/212375 [..............................] - ETA: 2326:05:06 - loss: 12.2584","output_type":"stream"}]},{"cell_type":"code","source":"from seaborn import lineplot\nfrom matplotlib import pyplot as plt\n\nlineplot(history.history['loss'])\nlineplot(history.history['val_loss'])\n\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:32:18.613770Z","iopub.status.idle":"2023-07-07T10:32:18.614597Z","shell.execute_reply.started":"2023-07-07T10:32:18.614309Z","shell.execute_reply":"2023-07-07T10:32:18.614332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"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\")\nmodel.push_to_hub('raygx/BertL-Nepali')\ntokenizer.push_to_hub('raygx/BertL-Nepali')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\n# Deleting dataset directory \ndir_path = r\"/kaggle/working/corpus\"\nshutil.rmtree(dir_path, ignore_errors=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T10:32:18.616056Z","iopub.status.idle":"2023-07-07T10:32:18.616869Z","shell.execute_reply.started":"2023-07-07T10:32:18.616614Z","shell.execute_reply":"2023-07-07T10:32:18.616637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Testing\nfrom transformers import FillMaskPipeline\n\n\ntokenizer('नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच हजार वर्ष अघिदेखि')\npipeline = FillMaskPipeline(model=model,tokenizer=tokenizer,device=1)\npipeline('नेपाली भान्सामा प्रयोग हुने सुगन्धित धनियाँ पाँच [MASK] वर्ष अघिदेखि')","metadata":{"execution":{"iopub.status.busy":"2023-05-27T23:42:08.865354Z","iopub.execute_input":"2023-05-27T23:42:08.865859Z","iopub.status.idle":"2023-05-27T23:42:09.195914Z","shell.execute_reply.started":"2023-05-27T23:42:08.86582Z","shell.execute_reply":"2023-05-27T23:42:09.19477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}